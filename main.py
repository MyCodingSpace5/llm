import mathdef sigmoid(x):    return (1/1 + -math.e ** -x)def cross_entropy(x: int, output: int):    delta = x - output    return math.log(delta)class NeuralNetwork:    bias: [int]    weights: [int]    features: [int]    max_range: int    pos: int    stride: int    padding: int    input_data: [int]    pooling_layer: [int]    feature_map: [int]    def __init__(self, bias, weights, features, max_range, input_data):        self.bias = bias        self.weights = weights        self.features = features        self.max_range = max_range        self.input_data = input_data    def forward_propgation():        nxt_entry = pos + 1        if nxt_entry > max_range:            return        features[nxt_entry] = features[pos] * weights[pos] + bias[pos]    def backpropgation(output: int, learning_rate):        for i in range(max_range - 1):            delta = cross_entropy(features[i+1], output)            self.weights[i] = self.weights[i] - learning_rate * delta/output            self.bias[i] = self.bias[i] - learning_rate * delta/output    def output(x: int):        output = [int]        for i in range(len(input_data)/stride):           output[i] = sum(weights) * inputdata[i] + padding    def max_pooling(x: int):        pixels = int        for i in range(len(input_data)/stride):            pixels = max(pixels, input_data) + padding        return pixelsclass FNN:    bias = [int]    weights = [int]    features = [int]    pos = int    max_range = int    def forward():        for i in range(max_range - 1):            next = i + 1            self.features[next] = self.features[i] * self.weights[i] + self.bias[i]    def backpropgation(output: int, learning_rate: int):        for i in range(max_range - 1):            delta = cross_entropy(features[i+=1], output)            self.weights[i] = self.weights[i] - learning_rate * output            self.bias[i] = self.bias[i] - learning_rate * outputdef main():    pass
if __name__ == "__main__":    main()class Transformer:    attention_weights: int    mask: int    def multiheadattention(query, value, key):        for q,v,k,w in query,value,key,self.attention_weights:            return (q * weights) + (v * weights) + (k * weights)    def attention_learning(output: int, learning_rate: int):        delta = cross_entropy(sum(attention_weights), output)        for weight in attention_weights:            weight = weight - learning_rate * delta/output    def masked_attention(x: int, mask: int, stride: int):        return x * (mask / stride)    def generate_mask(query: [int], value: [int], key:[int]):        mask = 0        for q,v,k in query,value,key:            mask += multiheadattention(q,v,k)        self.mask = math.log(mask/len(query))                
